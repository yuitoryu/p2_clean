{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f84011c-b490-4052-8ba2-40ffb03b3e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yb2055/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import RobertaModel, RobertaTokenizer, TrainingArguments, Trainer, DataCollatorWithPadding, RobertaForSequenceClassification\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from datasets import load_dataset, Dataset, ClassLabel\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1d7159b-57bf-471d-9839-24828f02620c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of labels: 4\n",
      "the labels: ['World', 'Sports', 'Business', 'Sci/Tech']\n"
     ]
    }
   ],
   "source": [
    "base_model = 'roberta-large'\n",
    "\n",
    "dataset = load_dataset('ag_news', split='train')\n",
    "tokenizer = RobertaTokenizer.from_pretrained(base_model)\n",
    "\n",
    "def preprocess(examples):\n",
    "    tokenized = tokenizer(examples['text'], truncation=True, padding=True)\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True,  remove_columns=[\"text\"])\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "# Extract the number of classess and their names\n",
    "num_labels = dataset.features['label'].num_classes\n",
    "class_names = dataset.features[\"label\"].names\n",
    "print(f\"number of labels: {num_labels}\")\n",
    "print(f\"the labels: {class_names}\")\n",
    "\n",
    "# Create an id2label mapping\n",
    "# We will need this for our classifier.\n",
    "id2label = {i: label for i, label in enumerate(class_names)}\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23ba9a23-2bf6-4387-8f9a-42d90c5362d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    base_model,\n",
    "    id2label=id2label)\n",
    "# Split the original training set\n",
    "split_datasets = tokenized_dataset.train_test_split(test_size=10000, seed=42)\n",
    "train_dataset = split_datasets['train']\n",
    "eval_dataset = split_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "819315bd-1c86-478b-888e-e56f4d1b7286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT Model\n",
      "trainable params: 4,248,580 || all params: 359,612,424 || trainable%: 1.1814\n"
     ]
    }
   ],
   "source": [
    "layer_indices = range(8, 24)\n",
    "target_modules_mid_upper = [f\"roberta.encoder.layer.{i}.attention.self.query\" for i in layer_indices]\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=32, # Good rank for this number of layers\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.2,\n",
    "    bias='none',\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    use_dora=True,\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    use_rslora=True\n",
    ")\n",
    "# peft_model.unload()\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "print('PEFT Model')\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06379e41-afbb-46c9-ad4b-a5691a3eee38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "# Setup Training args\n",
    "output_dir = \"teacher_qv_r32_rslora_ep10\"\n",
    "training_args = TrainingArguments(\n",
    "        bf16=True,\n",
    "        output_dir=output_dir,\n",
    "        report_to=None,\n",
    "        eval_strategy='steps',\n",
    "        eval_steps=5000,  #  More frequent eval\n",
    "        logging_steps=5000,  #  More frequent logging\n",
    "        learning_rate=5e-5,  # Increased learning rate\n",
    "        warmup_ratio=0.1,  # ✅ Warmup added\n",
    "        weight_decay=0.01,\n",
    "        num_train_epochs=10,  # ✅ Reduced from 20 to 10\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=64,\n",
    "        optim=\"adamw_torch\",  #  Switched from SGD to AdamW\n",
    "        lr_scheduler_type=\"cosine\",  #  Added cosine scheduler\n",
    "        dataloader_num_workers=8,\n",
    "        gradient_checkpointing=False,  #  Left disabled, matching current setup\n",
    "        gradient_checkpointing_kwargs={'use_reentrant':True},\n",
    "        save_strategy= \"steps\",      # Explicitly set strategy to steps\n",
    "        save_steps= 5000             # Save a checkpoint every 500 steps\n",
    "        \n",
    "    )\n",
    "\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class SavePeftModelCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, model=None, **kwargs):\n",
    "        # Save only the LoRA adapter weights\n",
    "        epoch = int(state.epoch)\n",
    "        save_path = f\"lora-ag_news_r22_ep{epoch}\"\n",
    "        model.save_pretrained(save_path)\n",
    "        print(f\"Saved LoRA weights at {save_path}\")\n",
    "\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class SimpleLoggerCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        with open(\"training_log2.txt\", \"a\") as f:\n",
    "            f.write(str(logs) + \"\\n\")\n",
    "\n",
    "\n",
    "def get_trainer(model):\n",
    "    return Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[SimpleLoggerCallback()]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99527d16-1902-4fab-b0df-160f9e827a43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='68750' max='68750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [68750/68750 3:14:05, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.362500</td>\n",
       "      <td>0.273098</td>\n",
       "      <td>0.926600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.220700</td>\n",
       "      <td>0.196186</td>\n",
       "      <td>0.942800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.192500</td>\n",
       "      <td>0.215722</td>\n",
       "      <td>0.939300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.179700</td>\n",
       "      <td>0.190760</td>\n",
       "      <td>0.948600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.163700</td>\n",
       "      <td>0.201213</td>\n",
       "      <td>0.949600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.153100</td>\n",
       "      <td>0.205372</td>\n",
       "      <td>0.947700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.142700</td>\n",
       "      <td>0.215248</td>\n",
       "      <td>0.949700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.125600</td>\n",
       "      <td>0.221721</td>\n",
       "      <td>0.951700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.116200</td>\n",
       "      <td>0.237414</td>\n",
       "      <td>0.950700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.105900</td>\n",
       "      <td>0.231821</td>\n",
       "      <td>0.951000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.095200</td>\n",
       "      <td>0.242312</td>\n",
       "      <td>0.952200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.085500</td>\n",
       "      <td>0.249915</td>\n",
       "      <td>0.952300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.077900</td>\n",
       "      <td>0.254521</td>\n",
       "      <td>0.951400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_lora_finetuning_trainer = get_trainer(peft_model)\n",
    "\n",
    "result = peft_lora_finetuning_trainer.train()\n",
    "peft_model.save_pretrained('qv_r32_rslora_ep10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "280d1445-ddae-4b48-b89c-f6dbd802ae26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT Model loaded.\n",
      "trainable params: 2,085,892 || all params: 357,449,736 || trainable%: 0.5835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yb2055/.local/lib/python3.9/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/home/yb2055/.local/lib/python3.9/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "base_model = 'roberta-base'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(base_model)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "smodel = RobertaForSequenceClassification.from_pretrained(\n",
    "    base_model,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label\n",
    ")\n",
    "peft_config = LoraConfig(\n",
    "    r=10,\n",
    "    lora_alpha=20,\n",
    "    lora_dropout=0.2,\n",
    "    bias = 'none',\n",
    "    use_dora=True,\n",
    "    target_modules = ['query','value'],\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    use_rslora=True\n",
    ")\n",
    "s_model = get_peft_model(model, peft_config)\n",
    "print(\"PEFT Model loaded.\")\n",
    "s_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2fbc079-1a61-4b00-af67-f0a5ae6f8341",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "peft_model.eval()\n",
    "class DistillationTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Custom Trainer that adds a distillation loss on top of the student’s CE loss.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        teacher_model,\n",
    "        alpha_distillation=0.7,\n",
    "        temperature=2.0,\n",
    "        *args, \n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher_model = teacher_model\n",
    "        self.alpha_distillation = alpha_distillation\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        \n",
    "        # Forward pass on student (LoRA) model\n",
    "        outputs_student = model(**inputs)\n",
    "        student_logits = outputs_student.logits\n",
    "        \n",
    "        # Forward pass on teacher (no grad)\n",
    "        with torch.no_grad():\n",
    "            outputs_teacher = self.teacher_model(**inputs)\n",
    "            teacher_logits = outputs_teacher.logits\n",
    "\n",
    "        # 1) Hard-label cross-entropy\n",
    "        loss_ce = F.cross_entropy(\n",
    "            student_logits.view(-1, self.model.config.num_labels),\n",
    "            labels.view(-1)\n",
    "        )\n",
    "\n",
    "        # 2) Distillation (KL-Div between teacher & student)\n",
    "        T = self.temperature\n",
    "        student_logits_T = student_logits / T\n",
    "        teacher_logits_T = teacher_logits / T\n",
    "        \n",
    "        loss_kl = F.kl_div(\n",
    "            F.log_softmax(student_logits_T, dim=-1),\n",
    "            F.softmax(teacher_logits_T, dim=-1),\n",
    "            reduction=\"batchmean\",\n",
    "        ) * (T * T)\n",
    "\n",
    "        # Combine them\n",
    "        loss = (self.alpha_distillation * loss_kl) + ((1 - self.alpha_distillation) * loss_ce)\n",
    "\n",
    "        if return_outputs:\n",
    "            return (loss, outputs_student)\n",
    "        return loss\n",
    "output_dir = \"qv_r10_rslora_ep10\"\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class SimpleLoggerCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        with open(output_dir+\"training_log.txt\", \"a\") as f:\n",
    "            f.write(str(logs) + \"\\n\")\n",
    "import torch.nn.functional as F\n",
    "training_args = TrainingArguments(\n",
    "    bf16=True,\n",
    "    output_dir=output_dir,\n",
    "    report_to=None,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=5000,\n",
    "    logging_steps=5000,\n",
    "    learning_rate=1e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=10,               # example\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    dataloader_num_workers=8,\n",
    "    gradient_checkpointing=False,\n",
    "    save_strategy= \"steps\",      # Explicitly set strategy to steps\n",
    "    save_steps= 5000             # Save a checkpoint every 500 steps\n",
    "        \n",
    "    \n",
    ")\n",
    "\n",
    "distill_trainer = DistillationTrainer(\n",
    "    teacher_model=peft_model,\n",
    "    alpha_distillation=0.7,       # how much distillation matters\n",
    "    temperature=2.0,             # softening factor\n",
    "    model=s_model,            # LoRA student\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[SimpleLoggerCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b50b115-d884-4a73-b50f-0884dd810278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63281' max='68750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63281/68750 4:12:21 < 21:48, 4.18 it/s, Epoch 9.20/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.141300</td>\n",
       "      <td>0.065849</td>\n",
       "      <td>0.927300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.077100</td>\n",
       "      <td>0.056011</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.066200</td>\n",
       "      <td>0.052294</td>\n",
       "      <td>0.941600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.061000</td>\n",
       "      <td>0.048055</td>\n",
       "      <td>0.947400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.056400</td>\n",
       "      <td>0.048637</td>\n",
       "      <td>0.945600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.053000</td>\n",
       "      <td>0.048515</td>\n",
       "      <td>0.946600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.051400</td>\n",
       "      <td>0.046175</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.047100</td>\n",
       "      <td>0.043100</td>\n",
       "      <td>0.951700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>0.044295</td>\n",
       "      <td>0.951900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.042000</td>\n",
       "      <td>0.042585</td>\n",
       "      <td>0.953400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.041100</td>\n",
       "      <td>0.042875</td>\n",
       "      <td>0.953700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.041599</td>\n",
       "      <td>0.953900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distill_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd6bb3c-a142-4f89-943f-b1b64a16542c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a82c79-1113-4ad3-b49e-e80f45418fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcd97b7-4985-4dd3-a8bc-263e79423eb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95893ab1-acfe-4c61-af05-ac058bd3e197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(model, tokenizer, text):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "    output = model(**inputs)\n",
    "\n",
    "    prediction = output.logits.argmax(dim=-1).item()\n",
    "\n",
    "    print(f'\\n Class: {prediction}, Label: {id2label[prediction]}, Text: {text}')\n",
    "    return id2label[prediction]\n",
    "\n",
    "classify( s_model, tokenizer, \"Kederis proclaims innocence Olympic champion Kostas Kederis today left hospital ahead of his date with IOC inquisitors claiming his ...\")\n",
    "classify( s_model, tokenizer, \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1b9346-9874-4387-ba38-015a090e5124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_model(inference_model, dataset, labelled=True, batch_size=8, data_collator=None):\n",
    "    \"\"\"\n",
    "    Evaluate a PEFT model on a dataset.\n",
    "\n",
    "    Args:\n",
    "        inference_model: The model to evaluate.\n",
    "        dataset: The dataset (Hugging Face Dataset) to run inference on.\n",
    "        labelled (bool): If True, the dataset includes labels and metrics will be computed.\n",
    "                         If False, only predictions will be returned.\n",
    "        batch_size (int): Batch size for inference.\n",
    "        data_collator: Function to collate batches. If None, the default collate_fn is used.\n",
    "\n",
    "    Returns:\n",
    "        If labelled is True, returns a tuple (metrics, predictions)\n",
    "        If labelled is False, returns the predictions.\n",
    "    \"\"\"\n",
    "    # Create the DataLoader\n",
    "    eval_dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=data_collator)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    inference_model.to(device)\n",
    "    inference_model.eval()\n",
    "\n",
    "    all_predictions = []\n",
    "    if labelled:\n",
    "        metric = evaluate.load('accuracy')\n",
    "\n",
    "    # Loop over the DataLoader\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        # Move each tensor in the batch to the device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = inference_model(**batch)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        all_predictions.append(predictions.cpu())\n",
    "\n",
    "        if labelled:\n",
    "            # Expecting that labels are provided under the \"labels\" key.\n",
    "            references = batch[\"labels\"]\n",
    "            metric.add_batch(\n",
    "                predictions=predictions.cpu().numpy(),\n",
    "                references=references.cpu().numpy()\n",
    "            )\n",
    "\n",
    "    # Concatenate predictions from all batches\n",
    "    all_predictions = torch.cat(all_predictions, dim=0)\n",
    "\n",
    "    if labelled:\n",
    "        eval_metric = metric.compute()\n",
    "        print(\"Evaluation Metric:\", eval_metric)\n",
    "        return eval_metric, all_predictions\n",
    "    else:\n",
    "        return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f584971-8a72-49ac-ba9a-c97e6aab7e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check evaluation accuracy\n",
    "_, _ = evaluate_model(s_model, eval_dataset, True, 8, data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee372b32-fc34-4ac4-b4c3-9c3497943c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load your unlabelled data\n",
    "unlabelled_dataset = pd.read_pickle(\"test_unlabelled.pkl\")\n",
    "test_dataset = unlabelled_dataset.map(preprocess, batched=True, remove_columns=[\"text\"])\n",
    "unlabelled_dataset\n",
    "\n",
    "# Run inference and save predictions\n",
    "preds = evaluate_model(peft_model, test_dataset, False, 8, data_collator)\n",
    "df_output = pd.DataFrame({\n",
    "    'ID': range(len(preds)),\n",
    "    'Label': preds.numpy()  # or preds.tolist()\n",
    "})\n",
    "df_output.to_csv(os.path.join(output_dir,\"inference_output_distill_alpha_7_from_qv_r32_rslora_ep10_qv_r10_loraout_2_rslora_ep10.csv\"), index=False)\n",
    "print(\"Inference complete. Predictions saved to inference_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f220c32c-4862-4075-9a51-3a00ff54f3dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
