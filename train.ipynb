{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f84011c-b490-4052-8ba2-40ffb03b3e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yb2055/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import RobertaModel, RobertaTokenizer, TrainingArguments, Trainer, DataCollatorWithPadding, RobertaForSequenceClassification\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from datasets import load_dataset, Dataset, ClassLabel\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1d7159b-57bf-471d-9839-24828f02620c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of labels: 4\n",
      "the labels: ['World', 'Sports', 'Business', 'Sci/Tech']\n"
     ]
    }
   ],
   "source": [
    "base_model = 'roberta-large'\n",
    "\n",
    "dataset = load_dataset('ag_news', split='train')\n",
    "tokenizer = RobertaTokenizer.from_pretrained(base_model)\n",
    "\n",
    "def preprocess(examples):\n",
    "    tokenized = tokenizer(examples['text'], truncation=True, padding=True)\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True,  remove_columns=[\"text\"])\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "# Extract the number of classess and their names\n",
    "num_labels = dataset.features['label'].num_classes\n",
    "class_names = dataset.features[\"label\"].names\n",
    "print(f\"number of labels: {num_labels}\")\n",
    "print(f\"the labels: {class_names}\")\n",
    "\n",
    "# Create an id2label mapping\n",
    "# We will need this for our classifier.\n",
    "id2label = {i: label for i, label in enumerate(class_names)}\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23ba9a23-2bf6-4387-8f9a-42d90c5362d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    base_model,\n",
    "    id2label=id2label)\n",
    "# Split the original training set\n",
    "split_datasets = tokenized_dataset.train_test_split(test_size=10000, seed=42)\n",
    "train_dataset = split_datasets['train']\n",
    "eval_dataset = split_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "819315bd-1c86-478b-888e-e56f4d1b7286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT Model\n",
      "trainable params: 4,248,580 || all params: 359,612,424 || trainable%: 1.1814\n"
     ]
    }
   ],
   "source": [
    "layer_indices = range(8, 24)\n",
    "target_modules_mid_upper = [f\"roberta.encoder.layer.{i}.attention.self.query\" for i in layer_indices]\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=32, # Good rank for this number of layers\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.2,\n",
    "    bias='none',\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    use_dora=True,\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "# peft_model.unload()\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "print('PEFT Model')\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06379e41-afbb-46c9-ad4b-a5691a3eee38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "# Setup Training args\n",
    "output_dir = \"teacher_qv_r32_ep10\"\n",
    "training_args = TrainingArguments(\n",
    "        bf16=True,\n",
    "        output_dir=output_dir,\n",
    "        report_to=None,\n",
    "        eval_strategy='steps',\n",
    "        eval_steps=1000,  #  More frequent eval\n",
    "        logging_steps=1000,  #  More frequent logging\n",
    "        learning_rate=5e-5,  # Increased learning rate\n",
    "        warmup_ratio=0.1,  #  Warmup added\n",
    "        weight_decay=0.01,\n",
    "        num_train_epochs=10,  #  Reduced from 20 to 10\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=64,\n",
    "        optim=\"adamw_torch\",  #  Switched from SGD to AdamW\n",
    "        lr_scheduler_type=\"cosine\",  #  Added cosine scheduler\n",
    "        dataloader_num_workers=8,\n",
    "        gradient_checkpointing=False,  #  Left disabled, matching current setup\n",
    "        gradient_checkpointing_kwargs={'use_reentrant':True},\n",
    "        save_strategy= \"steps\",      # Explicitly set strategy to steps\n",
    "        save_steps= 5000             # Save a checkpoint every 500 steps\n",
    ")\n",
    "    \n",
    "\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class SavePeftModelCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, model=None, **kwargs):\n",
    "        # Save only the LoRA adapter weights\n",
    "        epoch = int(state.epoch)\n",
    "        save_path = f\"lora-ag_news_r22_ep{epoch}\"\n",
    "        model.save_pretrained(save_path)\n",
    "        print(f\"Saved LoRA weights at {save_path}\")\n",
    "\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class SimpleLoggerCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        with open(\"training_log_teacher.txt\", \"a\") as f:\n",
    "            f.write(str(logs) + \"\\n\")\n",
    "\n",
    "\n",
    "def get_trainer(model):\n",
    "    return Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[SimpleLoggerCallback()]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99527d16-1902-4fab-b0df-160f9e827a43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='68750' max='68750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [68750/68750 3:38:34, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.954900</td>\n",
       "      <td>0.304670</td>\n",
       "      <td>0.903000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.285300</td>\n",
       "      <td>0.289089</td>\n",
       "      <td>0.910100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.275600</td>\n",
       "      <td>0.262544</td>\n",
       "      <td>0.917200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.254800</td>\n",
       "      <td>0.255051</td>\n",
       "      <td>0.919900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.245700</td>\n",
       "      <td>0.272679</td>\n",
       "      <td>0.921200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.219300</td>\n",
       "      <td>0.231146</td>\n",
       "      <td>0.930900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.217700</td>\n",
       "      <td>0.212030</td>\n",
       "      <td>0.938300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.205600</td>\n",
       "      <td>0.218871</td>\n",
       "      <td>0.933300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.198700</td>\n",
       "      <td>0.215307</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.201100</td>\n",
       "      <td>0.190865</td>\n",
       "      <td>0.939400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.194800</td>\n",
       "      <td>0.193149</td>\n",
       "      <td>0.943600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.180400</td>\n",
       "      <td>0.192389</td>\n",
       "      <td>0.943100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.186400</td>\n",
       "      <td>0.181391</td>\n",
       "      <td>0.944200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.178100</td>\n",
       "      <td>0.183080</td>\n",
       "      <td>0.942500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.176400</td>\n",
       "      <td>0.192303</td>\n",
       "      <td>0.943400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.172700</td>\n",
       "      <td>0.174520</td>\n",
       "      <td>0.945700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.159100</td>\n",
       "      <td>0.178765</td>\n",
       "      <td>0.946000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.170900</td>\n",
       "      <td>0.175594</td>\n",
       "      <td>0.945700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.165500</td>\n",
       "      <td>0.170216</td>\n",
       "      <td>0.946800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.167700</td>\n",
       "      <td>0.176153</td>\n",
       "      <td>0.948200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.162400</td>\n",
       "      <td>0.176334</td>\n",
       "      <td>0.944400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.148100</td>\n",
       "      <td>0.170731</td>\n",
       "      <td>0.947700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.157300</td>\n",
       "      <td>0.175712</td>\n",
       "      <td>0.949700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.150900</td>\n",
       "      <td>0.163250</td>\n",
       "      <td>0.948500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.149200</td>\n",
       "      <td>0.179747</td>\n",
       "      <td>0.948200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.147400</td>\n",
       "      <td>0.166927</td>\n",
       "      <td>0.949800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.149100</td>\n",
       "      <td>0.177878</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.139700</td>\n",
       "      <td>0.188689</td>\n",
       "      <td>0.946400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.137300</td>\n",
       "      <td>0.169725</td>\n",
       "      <td>0.950900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.143900</td>\n",
       "      <td>0.172990</td>\n",
       "      <td>0.949700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.173010</td>\n",
       "      <td>0.950700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.141200</td>\n",
       "      <td>0.171414</td>\n",
       "      <td>0.952200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.134100</td>\n",
       "      <td>0.171620</td>\n",
       "      <td>0.951600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.142000</td>\n",
       "      <td>0.162505</td>\n",
       "      <td>0.952600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.131600</td>\n",
       "      <td>0.175568</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.124600</td>\n",
       "      <td>0.175761</td>\n",
       "      <td>0.952500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.125400</td>\n",
       "      <td>0.166648</td>\n",
       "      <td>0.953200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.130200</td>\n",
       "      <td>0.175500</td>\n",
       "      <td>0.951700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.122800</td>\n",
       "      <td>0.180495</td>\n",
       "      <td>0.951800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.120800</td>\n",
       "      <td>0.175826</td>\n",
       "      <td>0.951000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.137100</td>\n",
       "      <td>0.171790</td>\n",
       "      <td>0.953700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.116000</td>\n",
       "      <td>0.178074</td>\n",
       "      <td>0.951000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.115900</td>\n",
       "      <td>0.173409</td>\n",
       "      <td>0.952200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.182036</td>\n",
       "      <td>0.952500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.116200</td>\n",
       "      <td>0.171505</td>\n",
       "      <td>0.953400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.114100</td>\n",
       "      <td>0.174763</td>\n",
       "      <td>0.953300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.119500</td>\n",
       "      <td>0.167528</td>\n",
       "      <td>0.954000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.115200</td>\n",
       "      <td>0.172256</td>\n",
       "      <td>0.954100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.104400</td>\n",
       "      <td>0.172168</td>\n",
       "      <td>0.953100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.108400</td>\n",
       "      <td>0.175425</td>\n",
       "      <td>0.954200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.110600</td>\n",
       "      <td>0.171133</td>\n",
       "      <td>0.954400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.110500</td>\n",
       "      <td>0.171694</td>\n",
       "      <td>0.953700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>0.106600</td>\n",
       "      <td>0.174795</td>\n",
       "      <td>0.954900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.107500</td>\n",
       "      <td>0.174700</td>\n",
       "      <td>0.953900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.104400</td>\n",
       "      <td>0.175057</td>\n",
       "      <td>0.954400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.105500</td>\n",
       "      <td>0.175033</td>\n",
       "      <td>0.954700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>0.105100</td>\n",
       "      <td>0.173051</td>\n",
       "      <td>0.953800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.095300</td>\n",
       "      <td>0.174336</td>\n",
       "      <td>0.954800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>0.104900</td>\n",
       "      <td>0.174745</td>\n",
       "      <td>0.954900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>0.175976</td>\n",
       "      <td>0.954900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>0.097400</td>\n",
       "      <td>0.175699</td>\n",
       "      <td>0.954700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.097800</td>\n",
       "      <td>0.175698</td>\n",
       "      <td>0.955400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>0.099400</td>\n",
       "      <td>0.175311</td>\n",
       "      <td>0.955000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.098600</td>\n",
       "      <td>0.175451</td>\n",
       "      <td>0.955300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.103000</td>\n",
       "      <td>0.175491</td>\n",
       "      <td>0.955000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.097500</td>\n",
       "      <td>0.175273</td>\n",
       "      <td>0.955200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>0.097500</td>\n",
       "      <td>0.175321</td>\n",
       "      <td>0.955300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.096600</td>\n",
       "      <td>0.175466</td>\n",
       "      <td>0.955400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_lora_finetuning_trainer = get_trainer(peft_model)\n",
    "\n",
    "result = peft_lora_finetuning_trainer.train()\n",
    "peft_model.save_pretrained('qv_r32_rslora_ep10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "280d1445-ddae-4b48-b89c-f6dbd802ae26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT Model loaded.\n",
      "trainable params: 2,085,892 || all params: 357,449,736 || trainable%: 0.5835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yb2055/.local/lib/python3.9/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/home/yb2055/.local/lib/python3.9/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "base_model = 'roberta-base'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(base_model)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "smodel = RobertaForSequenceClassification.from_pretrained(\n",
    "    base_model,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label\n",
    ")\n",
    "peft_config = LoraConfig(\n",
    "    r=10,\n",
    "    lora_alpha=20,\n",
    "    lora_dropout=0.2,\n",
    "    bias = 'none',\n",
    "    use_dora=True,\n",
    "    target_modules = ['query','value'],\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    use_rslora=True\n",
    ")\n",
    "s_model = get_peft_model(model, peft_config)\n",
    "print(\"PEFT Model loaded.\")\n",
    "s_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2fbc079-1a61-4b00-af67-f0a5ae6f8341",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "peft_model.eval()\n",
    "class DistillationTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Custom Trainer that adds a distillation loss on top of the student’s CE loss.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        teacher_model,\n",
    "        alpha_distillation=0.7,\n",
    "        temperature=2.0,\n",
    "        *args, \n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher_model = teacher_model\n",
    "        self.alpha_distillation = alpha_distillation\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        \n",
    "        # Forward pass on student (LoRA) model\n",
    "        outputs_student = model(**inputs)\n",
    "        student_logits = outputs_student.logits\n",
    "        \n",
    "        # Forward pass on teacher (no grad)\n",
    "        with torch.no_grad():\n",
    "            outputs_teacher = self.teacher_model(**inputs)\n",
    "            teacher_logits = outputs_teacher.logits\n",
    "\n",
    "        # 1) Hard-label cross-entropy\n",
    "        loss_ce = F.cross_entropy(\n",
    "            student_logits.view(-1, self.model.config.num_labels),\n",
    "            labels.view(-1)\n",
    "        )\n",
    "\n",
    "        # 2) Distillation (KL-Div between teacher & student)\n",
    "        T = self.temperature\n",
    "        student_logits_T = student_logits / T\n",
    "        teacher_logits_T = teacher_logits / T\n",
    "        \n",
    "        loss_kl = F.kl_div(\n",
    "            F.log_softmax(student_logits_T, dim=-1),\n",
    "            F.softmax(teacher_logits_T, dim=-1),\n",
    "            reduction=\"batchmean\",\n",
    "        ) * (T * T)\n",
    "\n",
    "        # Combine them\n",
    "        loss = (self.alpha_distillation * loss_kl) + ((1 - self.alpha_distillation) * loss_ce)\n",
    "\n",
    "        if return_outputs:\n",
    "            return (loss, outputs_student)\n",
    "        return loss\n",
    "output_dir = \"qv_r10_rslora_ep10\"\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class SimpleLoggerCallbackStudent(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        with open(output_dir+\"training_log_student.txt\", \"a\") as f:\n",
    "            f.write(str(logs) + \"\\n\")\n",
    "import torch.nn.functional as F\n",
    "training_args = TrainingArguments(\n",
    "    bf16=True,\n",
    "    output_dir=output_dir,\n",
    "    report_to=None,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=1000,\n",
    "    logging_steps=1000,\n",
    "    learning_rate=1e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=10,               # example\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    dataloader_num_workers=8,\n",
    "    gradient_checkpointing=False,\n",
    "    save_strategy= \"steps\",      # Explicitly set strategy to steps\n",
    "    save_steps= 5000             # Save a checkpoint every 500 steps\n",
    "    \n",
    "        \n",
    "    \n",
    ")\n",
    "\n",
    "distill_trainer = DistillationTrainer(\n",
    "    teacher_model=peft_model,\n",
    "    alpha_distillation=0.7,       # how much distillation matters\n",
    "    temperature=2.0,             # softening factor\n",
    "    model=s_model,            # LoRA student\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[SimpleLoggerCallbackStudent()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b50b115-d884-4a73-b50f-0884dd810278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='49386' max='68750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [49386/68750 3:52:52 < 1:31:18, 3.53 it/s, Epoch 7.18/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.268100</td>\n",
       "      <td>0.081525</td>\n",
       "      <td>0.909800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.125100</td>\n",
       "      <td>0.078742</td>\n",
       "      <td>0.913300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.103000</td>\n",
       "      <td>0.069608</td>\n",
       "      <td>0.918800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.095700</td>\n",
       "      <td>0.066660</td>\n",
       "      <td>0.923800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.089400</td>\n",
       "      <td>0.063769</td>\n",
       "      <td>0.929000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.083400</td>\n",
       "      <td>0.071349</td>\n",
       "      <td>0.917500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.079800</td>\n",
       "      <td>0.059445</td>\n",
       "      <td>0.934900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.076100</td>\n",
       "      <td>0.058512</td>\n",
       "      <td>0.935400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.073800</td>\n",
       "      <td>0.056932</td>\n",
       "      <td>0.937600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.073200</td>\n",
       "      <td>0.056246</td>\n",
       "      <td>0.936600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.069500</td>\n",
       "      <td>0.052858</td>\n",
       "      <td>0.940100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.066600</td>\n",
       "      <td>0.052548</td>\n",
       "      <td>0.940900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.067700</td>\n",
       "      <td>0.053560</td>\n",
       "      <td>0.939800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.064500</td>\n",
       "      <td>0.050993</td>\n",
       "      <td>0.942200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.064600</td>\n",
       "      <td>0.056999</td>\n",
       "      <td>0.936100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.062900</td>\n",
       "      <td>0.050599</td>\n",
       "      <td>0.944600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.060200</td>\n",
       "      <td>0.051483</td>\n",
       "      <td>0.943800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.062100</td>\n",
       "      <td>0.051071</td>\n",
       "      <td>0.942100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.058500</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>0.943000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.060400</td>\n",
       "      <td>0.047845</td>\n",
       "      <td>0.947600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.057500</td>\n",
       "      <td>0.050417</td>\n",
       "      <td>0.944300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.055800</td>\n",
       "      <td>0.050069</td>\n",
       "      <td>0.944400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.059000</td>\n",
       "      <td>0.051069</td>\n",
       "      <td>0.945800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.055300</td>\n",
       "      <td>0.047498</td>\n",
       "      <td>0.947500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.054900</td>\n",
       "      <td>0.049105</td>\n",
       "      <td>0.947700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.055400</td>\n",
       "      <td>0.049122</td>\n",
       "      <td>0.946400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.054500</td>\n",
       "      <td>0.047344</td>\n",
       "      <td>0.947300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.052400</td>\n",
       "      <td>0.050280</td>\n",
       "      <td>0.945300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.051800</td>\n",
       "      <td>0.047124</td>\n",
       "      <td>0.948200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.052300</td>\n",
       "      <td>0.049346</td>\n",
       "      <td>0.943900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.049900</td>\n",
       "      <td>0.045988</td>\n",
       "      <td>0.949100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.052400</td>\n",
       "      <td>0.045484</td>\n",
       "      <td>0.949200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.049300</td>\n",
       "      <td>0.044797</td>\n",
       "      <td>0.949800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.050300</td>\n",
       "      <td>0.044765</td>\n",
       "      <td>0.950700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.049600</td>\n",
       "      <td>0.043724</td>\n",
       "      <td>0.952000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.048000</td>\n",
       "      <td>0.046853</td>\n",
       "      <td>0.950900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.048600</td>\n",
       "      <td>0.044435</td>\n",
       "      <td>0.950400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.049600</td>\n",
       "      <td>0.044600</td>\n",
       "      <td>0.952000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.046700</td>\n",
       "      <td>0.045796</td>\n",
       "      <td>0.949400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.045800</td>\n",
       "      <td>0.043204</td>\n",
       "      <td>0.954500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.050700</td>\n",
       "      <td>0.043290</td>\n",
       "      <td>0.951300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.044200</td>\n",
       "      <td>0.042692</td>\n",
       "      <td>0.952800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.045400</td>\n",
       "      <td>0.043761</td>\n",
       "      <td>0.951500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>0.043329</td>\n",
       "      <td>0.951700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.043600</td>\n",
       "      <td>0.042374</td>\n",
       "      <td>0.951700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.043200</td>\n",
       "      <td>0.042611</td>\n",
       "      <td>0.953600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.044200</td>\n",
       "      <td>0.041364</td>\n",
       "      <td>0.954600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.042100</td>\n",
       "      <td>0.041897</td>\n",
       "      <td>0.954200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.041629</td>\n",
       "      <td>0.954400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distill_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95893ab1-acfe-4c61-af05-ac058bd3e197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(model, tokenizer, text):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "    output = model(**inputs)\n",
    "\n",
    "    prediction = output.logits.argmax(dim=-1).item()\n",
    "\n",
    "    print(f'\\n Class: {prediction}, Label: {id2label[prediction]}, Text: {text}')\n",
    "    return id2label[prediction]\n",
    "\n",
    "classify( s_model, tokenizer, \"Kederis proclaims innocence Olympic champion Kostas Kederis today left hospital ahead of his date with IOC inquisitors claiming his ...\")\n",
    "classify( s_model, tokenizer, \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1b9346-9874-4387-ba38-015a090e5124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_model(inference_model, dataset, labelled=True, batch_size=8, data_collator=None):\n",
    "    \"\"\"\n",
    "    Evaluate a PEFT model on a dataset.\n",
    "\n",
    "    Args:\n",
    "        inference_model: The model to evaluate.\n",
    "        dataset: The dataset (Hugging Face Dataset) to run inference on.\n",
    "        labelled (bool): If True, the dataset includes labels and metrics will be computed.\n",
    "                         If False, only predictions will be returned.\n",
    "        batch_size (int): Batch size for inference.\n",
    "        data_collator: Function to collate batches. If None, the default collate_fn is used.\n",
    "\n",
    "    Returns:\n",
    "        If labelled is True, returns a tuple (metrics, predictions)\n",
    "        If labelled is False, returns the predictions.\n",
    "    \"\"\"\n",
    "    # Create the DataLoader\n",
    "    eval_dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=data_collator)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    inference_model.to(device)\n",
    "    inference_model.eval()\n",
    "\n",
    "    all_predictions = []\n",
    "    if labelled:\n",
    "        metric = evaluate.load('accuracy')\n",
    "\n",
    "    # Loop over the DataLoader\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        # Move each tensor in the batch to the device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = inference_model(**batch)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        all_predictions.append(predictions.cpu())\n",
    "\n",
    "        if labelled:\n",
    "            # Expecting that labels are provided under the \"labels\" key.\n",
    "            references = batch[\"labels\"]\n",
    "            metric.add_batch(\n",
    "                predictions=predictions.cpu().numpy(),\n",
    "                references=references.cpu().numpy()\n",
    "            )\n",
    "\n",
    "    # Concatenate predictions from all batches\n",
    "    all_predictions = torch.cat(all_predictions, dim=0)\n",
    "\n",
    "    if labelled:\n",
    "        eval_metric = metric.compute()\n",
    "        print(\"Evaluation Metric:\", eval_metric)\n",
    "        return eval_metric, all_predictions\n",
    "    else:\n",
    "        return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f584971-8a72-49ac-ba9a-c97e6aab7e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check evaluation accuracy\n",
    "_, _ = evaluate_model(s_model, eval_dataset, True, 8, data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee372b32-fc34-4ac4-b4c3-9c3497943c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load your unlabelled data\n",
    "unlabelled_dataset = pd.read_pickle(\"test_unlabelled.pkl\")\n",
    "test_dataset = unlabelled_dataset.map(preprocess, batched=True, remove_columns=[\"text\"])\n",
    "unlabelled_dataset\n",
    "\n",
    "# Run inference and save predictions\n",
    "preds = evaluate_model(peft_model, test_dataset, False, 8, data_collator)\n",
    "df_output = pd.DataFrame({\n",
    "    'ID': range(len(preds)),\n",
    "    'Label': preds.numpy()  # or preds.tolist()\n",
    "})\n",
    "df_output.to_csv(os.path.join(output_dir,\"inference_output_distill_alpha_7_from_qv_r32_rslora_ep10_qv_r10_loraout_2_rslora_ep10.csv\"), index=False)\n",
    "print(\"Inference complete. Predictions saved to inference_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f220c32c-4862-4075-9a51-3a00ff54f3dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
